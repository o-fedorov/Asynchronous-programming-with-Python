{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous programming with Python\n",
    "## Module 4 - Compare asynchronous code approaches\n",
    "\n",
    "### Agenda:\n",
    "\n",
    "* Web crawler - find the shortest path between Wikipedia articles.\n",
    "    * Synchronous code.\n",
    "    * Multithreading.\n",
    "    * Trio.\n",
    "    * AsyncIO.\n",
    "* Multiprocessing - calculate Ï€ value with Monte-Carlo method.\n",
    "    * Synchronous code.\n",
    "    * Trio.\n",
    "    * AsyncIO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the shortest path between Wikipedia articles\n",
    "The task have following constraints:\n",
    "\n",
    "1. No more than 10 workers simultaneously.\n",
    "2. Each worker should sleep for some time in range 0.5-1.5 seconds\n",
    "   to prevent requests throttling.\n",
    "3. Stop if the path length exceeds 10 articles.\n",
    "4. Search only Ukrainian Wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import re\n",
    "!pip install trio asks requests aiohttp beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preparation\n",
    "First define some common code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Iterable\n",
    "from urllib.parse import unquote\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://uk.wikipedia.org\"\n",
    "MAX_WORKERS = 10\n",
    "MAX_PATH_LEN = 10\n",
    "log = logging.getLogger(\"main\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"The page URL to process.\"\"\"\n",
    "    url: str\n",
    "    path: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Page:\n",
    "    \"\"\"Processed page.\"\"\"\n",
    "    title: str\n",
    "    links: List[str] = field(default_factory=list)\n",
    "\n",
    "    @classmethod\n",
    "    def from_html(cls, text: str) -> Page:\n",
    "        \"\"\"Parse HTML text and provide the page info.\"\"\"\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        title = soup.find(id=\"firstHeading\")\n",
    "        all_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "\n",
    "        hrefs = []\n",
    "        for link in all_links:\n",
    "            href = unquote(link.attrs.get(\"href\", \"\"))\n",
    "            if (href.startswith(\"/wiki/\")\n",
    "                    # Exclude special pages\n",
    "                    and \":\" not in href\n",
    "                    # Exclude \"Ð’Ñ–ÐºÑ–ÑÑ…Ð¾Ð²Ð¸Ñ‰Ðµ\", \"Ð’Ñ–ÐºÑ–Ñ†Ð¸Ñ‚Ð°Ñ‚Ð¸\" and so on.\n",
    "                    and not href.startswith(\"/wiki/Ð’Ñ–ÐºÑ–\")\n",
    "                    # Exclude links that look like dates\n",
    "                    and not re.match(r\"/wiki/\\d.*\", href)):\n",
    "                hrefs.append(BASE_URL + href)\n",
    "\n",
    "        return cls(title.text, hrefs)\n",
    "\n",
    "\n",
    "def get_delay() -> float:\n",
    "    \"\"\"How long the worker should wait, seconds.\"\"\"\n",
    "    return random.random() + 0.5\n",
    "\n",
    "\n",
    "def print_result(path: Optional[List[str]]):\n",
    "    \"\"\"Print the result of the search.\"\"\"\n",
    "    if path:\n",
    "        print(\"The result is:\", \" -> \".join(path + [\".\"]))\n",
    "    else:\n",
    "        print(\"No route was found.\")\n",
    "\n",
    "\n",
    "def find_target_task(tasks: Iterable[Task], target_url) -> Optional[Task]:\n",
    "    \"\"\"Find a task that leads to the target URL, if any.\"\"\"\n",
    "    for task in tasks:\n",
    "        if task.url == target_url:\n",
    "            return task\n",
    "\n",
    "\n",
    "def log_config(level=logging.DEBUG):\n",
    "    \"\"\"Configure logging.\"\"\"\n",
    "    logging.basicConfig(format=\"%(message)s\")\n",
    "    log.setLevel(level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "The URLs are supposed to be the same for all implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "START_URL = \"https://uk.wikipedia.org/wiki/Ð¢Ñ€Ñ–Ð½Ñ–Ð»ÑŒÑÑŒÐºÐ¸Ð¹_Ñ‚Ð¸Ð³Ñ€\"\n",
    "TARGET_URL = \"https://uk.wikipedia.org/wiki/Ð‘Ð»Ð¸ÑÐºÐ°Ð²ÐºÐ°\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronous code\n",
    "Let's apply\n",
    "[Breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search)\n",
    "algorithm.\n",
    "\n",
    "We visit a page, add all found links to the queue, and then pop the\n",
    "links from the queue one by one and, if they were not visited before,\n",
    "visit them and start over.\n",
    "\n",
    "The synchronous implementation is quite slow.  It takes lots of time to\n",
    "just process the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def main(url, target_url, max_path_len=MAX_PATH_LEN):\n",
    "    \"\"\"Synchronously crawl Wikipedia in order to find the path.\"\"\"\n",
    "    seen = set()\n",
    "    queue = deque([Task(url)])\n",
    "\n",
    "    while queue:\n",
    "        # Some optimization to safe the calls\n",
    "        target_task = find_target_task(queue, target_url)\n",
    "        if target_task:\n",
    "            return target_task.path\n",
    "\n",
    "        task: Task = queue.popleft()\n",
    "\n",
    "        if task.url in seen or len(task.path) >= max_path_len:\n",
    "            continue\n",
    "\n",
    "        log.info(\"Processing %s\", task)\n",
    "        seen.add(task.url)\n",
    "        page: Page = process_page(task.url)\n",
    "        new_path = task.path + [page.title]\n",
    "\n",
    "        log.info(\"Adding %s ulrs.\", len(page.links))\n",
    "        tasks = [Task(url, new_path) for url in page.links]\n",
    "        queue.extend(tasks)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_page(url: str) -> Page:\n",
    "    \"\"\"Request the page contents.\"\"\"\n",
    "    time.sleep(get_delay())\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Page.from_html(resp.text)\n",
    "\n",
    "\n",
    "log_config()\n",
    "print_result(main(START_URL, TARGET_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading\n",
    "First refactor the code a little bit, to extract the `main`\n",
    "functionality for a separate page.\n",
    "\n",
    "It can look like this:\n",
    "\n",
    "```python\n",
    "def main(url, target_url, max_path_len=MAX_PATH_LEN):\n",
    "    \"\"\"Synchronously crawl Wikipedia in order to find the path.\"\"\"\n",
    "    seen = set()\n",
    "    queue = [Task(url)]\n",
    "\n",
    "    while True:\n",
    "        current_tasks = []\n",
    "\n",
    "        for task in queue:\n",
    "            if task.url == target_url:\n",
    "                return task.path\n",
    "\n",
    "            if task.url in seen or len(task.path) >= max_path_len:\n",
    "                continue\n",
    "            current_tasks.append(task)\n",
    "            seen.add(task.url)\n",
    "\n",
    "        if not current_tasks:\n",
    "            return None\n",
    "\n",
    "        queue.clear()\n",
    "        for task in current_tasks:\n",
    "            tasks = process_task(task)\n",
    "            queue.extend(tasks)\n",
    "\n",
    "\n",
    "def process_task(task: Task) -> List[Task]:\n",
    "    log.info(\"Processing %s\", task)\n",
    "    page: Page = process_page(task.url)\n",
    "    new_path = task.path + [page.title]\n",
    "\n",
    "    log.info(\"Adding %s ulrs.\", len(page.links))\n",
    "    return [Task(url, new_path) for url in page.links]\n",
    "\n",
    "```\n",
    "\n",
    "Now the last part can be executed with\n",
    "[concurrent.futures.ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor).\n",
    "See how it works much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def main(url, target_url, max_path_len=MAX_PATH_LEN):\n",
    "    \"\"\"Synchronously crawl Wikipedia in order to find the path.\"\"\"\n",
    "    seen = set()\n",
    "    queue = [Task(url)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        while True:\n",
    "            current_tasks = []\n",
    "\n",
    "            for task in queue:\n",
    "                if task.url == target_url:\n",
    "                    return task.path\n",
    "\n",
    "                if task.url in seen or len(task.path) >= max_path_len:\n",
    "                    continue\n",
    "                current_tasks.append(task)\n",
    "                seen.add(task.url)\n",
    "\n",
    "            if not current_tasks:\n",
    "                return None\n",
    "\n",
    "            queue.clear()\n",
    "            for tasks in executor.map(process_task, current_tasks):\n",
    "                queue.extend(tasks)\n",
    "\n",
    "\n",
    "def process_task(task: Task):\n",
    "    log.info(\"Processing %s\", task)\n",
    "    page: Page = process_page(task.url)\n",
    "    new_path = task.path + [page.title]\n",
    "\n",
    "    log.info(\"Adding %s ulrs.\", len(page.links))\n",
    "    return [Task(url, new_path) for url in page.links]\n",
    "\n",
    "\n",
    "def process_page(url: str) -> Page:\n",
    "    \"\"\"Request the page contents.\"\"\"\n",
    "    time.sleep(get_delay())\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Page.from_html(resp.text)\n",
    "\n",
    "\n",
    "log_config()\n",
    "print_result(main(START_URL, TARGET_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ *It is possible to use producer-consumer pattern with multithreading,\n",
    "though using ThreadPoolExecutor here is much simpler,\n",
    "while giving similar performance gain.*ðŸ‘ˆ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Set, Optional\n",
    "\n",
    "import asks\n",
    "import trio\n",
    "\n",
    "@dataclass\n",
    "class TaskContext:\n",
    "    \"\"\"Common context for the workers.\"\"\"\n",
    "    target_url: str\n",
    "    max_path_len: int\n",
    "    result: Optional[List[str]] = None\n",
    "    seen: Set[str] = field(default_factory=set)\n",
    "\n",
    "\n",
    "async def main(url: str, target_url: str, max_path_len: int = MAX_PATH_LEN):\n",
    "    context = TaskContext(target_url=target_url, max_path_len=max_path_len)\n",
    "    task = Task(url)\n",
    "\n",
    "    send_channel, receive_channel = trio.open_memory_channel(math.inf)\n",
    "    active_workers = trio.CapacityLimiter(MAX_WORKERS)\n",
    "\n",
    "    async with trio.open_nursery() as nursery:\n",
    "        async with send_channel, receive_channel:\n",
    "            send_channel.send_nowait(task)\n",
    "\n",
    "            for _ in range(MAX_WORKERS):\n",
    "                nursery.start_soon(\n",
    "                    crawler, context, active_workers, send_channel, receive_channel,\n",
    "                )\n",
    "\n",
    "            while True:\n",
    "                # Give the workers a chance to start up.\n",
    "                await trio.sleep(1)\n",
    "                if active_workers.borrowed_tokens == 0 and send_channel.statistics().current_buffer_used == 0:\n",
    "                    # All done!\n",
    "                    nursery.cancel_scope.cancel()\n",
    "                    break\n",
    "\n",
    "\n",
    "async def crawler(context: TaskContext, active_workers, send_channel, receive_channel):\n",
    "    task: Task\n",
    "\n",
    "    async for task in receive_channel:\n",
    "        async with active_workers:\n",
    "            if context.result:\n",
    "                return\n",
    "\n",
    "            if task.url == context.target_url:\n",
    "                context.result = task.path\n",
    "                return\n",
    "\n",
    "            if task.url in context.seen or len(task.path) >= context.max_path_len:\n",
    "                continue\n",
    "\n",
    "            log.info(\"Processing %s\", task)\n",
    "            context.seen.add(task.url)\n",
    "            page: Page = await process_page(task.url)\n",
    "\n",
    "            new_path = task.path + [page.title]\n",
    "            tasks = [Task(url, new_path) for url in page.links]\n",
    "\n",
    "            target_task = find_target_task(tasks, context.target_url)\n",
    "            if target_task:\n",
    "                context.result = new_path\n",
    "                return\n",
    "\n",
    "            log.info(\"Adding %s ulrs.\", len(tasks))\n",
    "            for next_task in tasks:\n",
    "                send_channel.send_nowait(next_task)\n",
    "\n",
    "\n",
    "\n",
    "async def process_page(url: str) -> Page:\n",
    "    \"\"\"Request the page contents.\"\"\"\n",
    "    await trio.sleep(get_delay())\n",
    "    resp = await asks.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Page.from_html(resp.text)\n",
    "\n",
    "\n",
    "log_config()\n",
    "print_result(trio.run(main, START_URL, TARGET_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: x-large\">Add your code below:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module2-Asynchronous-programming-with-Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}